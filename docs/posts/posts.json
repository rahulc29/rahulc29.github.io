[
  {
    "path": "posts/2022-02-24-the-beauty-and-applicability-of-algebraic-types/",
    "title": "The beauty and applicability of algebraic types",
    "description": "In this article we explore the wide applicability of algebraic types and how to use them in daily life",
    "author": [
      {
        "name": "Rahul Chhabra",
        "url": "https://rahulc29.netlify.app/"
      }
    ],
    "date": "2022-02-24",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nSyntax and elementary\nnotions\nLinked Lists as algebraic\ntypes\nElementary\noperations on algebraic linked lists\nList Sizes\nSum and\nproduct of an algebraic linked list\n\n\n\nIntroduction\nThe notion of a type is particularly important in\nprogramming. A significant portion of online discourse regarding the\nmerits and demerits of a given programming language comes down to if the\nlanguage‚Äôs type system:\nPython is dynamic and strong\nJavaScript is dynamic and weak (3 ==\n\"3\" but 3 !== \"3\")\nC is static and weak\nC++ and Java are static and strong\nKotlin, Rust and Haskell are static and very\nstrong\nOf all the programming languages aforementioned, only the last three\ncontain a rather powerful means of creating and composing types known as\nalgebraic types. It would be unfair to not mention the fact\nthat Java and C++\nare both proposing to add algebraic types to their type systems.\nSyntax and elementary\nnotions\nAt a very fundamental level, the notion of doing ‚Äúalgebra‚Äù with types\ncomes down to the following two operations :\nDisjoint Union : the union of two disjoint types\nensures that if we are given any instance of that type, we know it‚Äôs\n‚Äúparent type‚Äù\nCartesian Product : the Cartesian Product is of\ntypes A and B is the tuple (A,\nB)\nMost programming languages explicitly or implicitly contain the\nnotion of the Cartesian product.\nThis is all very abstract, so let‚Äôs try to make something using only\nthese operations : a linked list!\nLinked Lists as algebraic\ntypes\nConsider the following (singly) linked list :\n1 -> 2 -> 3 -> 4 -> 5\nObserve that any node can be thought of as containing\nanother linked list inside it, as in :\n1 -> (2 -> (3 -> (4 -> 5))\nWhat about the 5 though?\nLet us play mathematician and assume that the node\ncontaining 5 contains the empty list inside\nit.\nIs this legal?\nWell, we make the rules around here! ü§´\nSo our original list now becomes :\n1 -> (2 -> (3 -> (4 -> (5 -> []))))\nwhere [] represents the empty list.\nWhat is the point of doing that however?\nWe can now make the following claim, any linked list can be\neither of two things :\nEmpty : if the list is empty, it is\n[]\nNode : a node will have a value (an\nint in this example), and another list, this other list\nitself follows these same constraints obviously\nI‚Äôd like to know throw some terminology : the type of\nLinkedList may be defined as the disjoint union of\nthe EmptyList type and the ListNode type.\nMake another observation : the ListNode itself is the\nCartesian product of int and the\nLinkedList type.\nLet us write this down in code, shall we?\ntype LinkedList = Empty | Node of (int * LinkedList)\nLet‚Äôs break this down : a LinkedList is either an\nEmpty instance, in which case it holds no further\ninformation (this definition automatically implies that there is\nonly one empty list) or it is a Node consisting of\nthe Cartesian product of int and\nLinkedList\nNow that we have a linked list, let us try to do things with it!\nElementary\noperations on algebraic linked lists\nList Sizes\nThe first problem we will tackle is to compute the length of our\nlinked list. With out algebraic defintion, this is practically a\njoke:\nEmpty : define the size to be 0\nNode : define the size to be 1 added\nto the size of the contained list\nLet us write this down in code :\nlet rec size list = \n    match list with \n    | Empty -> 0 \n    | Node(value, containedList) -> 1 + size(containedList)\nThe match and with is special syntax used\nfor dealing with algebraic types called pattern matching. Every\nlanguage that provides algebraic types also provides some variant of\npattern matching.\nAs an example, let us take our old example and dry run it.\nWe have the list :\nlet a = 1 -> (2 -> (3 -> (4 -> (5 -> []))))\nlet b = a.tail // (2 -> (3 -> (4 -> (5 -> []))))\nlet c = b.tail // (3 -> (4 -> (5 -> [])))\nlet d = c.tail // (4 -> (5 -> []))\nlet e = d.tail // (5 -> [])\nlet f = e.tail // []\nWe have to compute size(a)\nThe execution would look something like this :\nsize(a)\n= 1 + size(b)\n= 1 + 1 + size(c)\n= 1 + 1 + 1 + size(d)\n= 1 + 1 + 1 + 1 + size(e)\n= 1 + 1 + 1 + 1 + 1 + size(f)\n= 1 + 1 + 1 + 1 + 1 + 0 \n= 5\nNote that this algorithm takes stack space that is linear\nwith the size of the list.\nHere‚Äôs an alternative implementation :\nlet size list = \n    // local function\n    let iter list result = \n        match list with \n        | Empty -> result \n        | Node(value, containedList) -> iter(containedList, result + 1)\n    iter(list, 0)\nThe stack space of this algorithm is constant with the size of the\nlist.\nHere‚Äôs a challenge for you : can you implement this algorithm in C++\nwithout using recursion but while loops instead? (a close approximation\nto this can be achieved by using an enum class and\nperforming switch case)\nSum and product of\nan algebraic linked list\nLet us compute the sum of an algebraic linked list.\nI urge you to come up with a solution yourself first!\nHere‚Äôs one way to do it :\nlet rec sum list = \n    match list with \n    | Empty -> 0 \n    | Node(value, containedList) -> value + sum(containedList)\nBeauty lies in the eyes of the beholder, but you need an eye checkup\nif you don‚Äôt consider this beautiful!\nVery similarly, we can compute the product of a list :\nlet rec product list = \n    match list with \n    | Empty -> 1 // this might not make much sense, but is actually the same thing as saying that the sum of the empty list is 0\n    | Node(value, containedList) -> value * product(containedList)\nThe two codes are awfully similar, let us generalise :\nwe need an identity to associate with the empty list\nwe need an operation to associate the value from the\ncomputation to our node‚Äôs value\nLet us write a general procedure that achieves this :\nlet rec reduce list operation identity = \n    match list with \n    | Empty -> identity \n    | Node(value, containedList) -> operation(value, containedList)\nThis is nice and all but a function with three arguments of which two\nclearly share a semantic relationship is extremely ugly!\nHow do we fix this?\nThe Cartesian product!\ntype BinaryOperation =  int * (int -> int -> int)\nThe int -> int -> int syntax is used to denote a\nfunction that takes two ints and returns an\nint.\nIf you are curious about the origin of such a syntax : look up the\nconcept of currying, you will probably enjoy it!\nThis is still not particularly descriptive : the type does not talk\nabout what the int and int -> int -> int\nare for.\nTo mitigate that, we can give them names :\ntype BinaryOperation = \n    { Identity: int,\n      Operation: int -> int -> int }\nThis might look like structs from other languages such\nas C/C++ or Rust.\nIt acts very similar to a struct and is called a\nrecord.\nThe only place in which records differ from structs is\nthat records can be natively recursive while structs can be\nimplemented recursively using some kind of pointers.\nI say ‚Äúsome kind of‚Äù because both C++ and Rust allow for very exotic\nkinds of pointers that would all work; think\nstd::unique_ptr<T>,\nstd::shared_ptr<T>,\nstd::weak_ptr<T>.\nIn Rust, one has even more control : for example, with\nBox<T> one can enforce that T\nbe on the heap. No such analog exists in C++.\nI digress, let us reimplement our reduce function!\nlet rec reduce list op = \n    match list with \n    | Empty -> op.Identity\n    | Node(value, containedList) -> op.Operation(value, reduce(containedList, op))\nlet Addition = BinaryOperation {\n    Identity : 0,\n    Operation : fun a b -> (a + b)\n}\nlet Multiplication = BinaryOperation {\n    Identity : 1,\n    Operation : fun a b -> (a * b)\n}\nlet sum list = reduce(list, Addition)\nlet product list = reduce(list, Multiplication)\nThis is still not reduces final form.\nObserve that the stack space grows linearly with list size.\nHere‚Äôs a mitigation\nlet reduce list op = \n    let rec iter list result = \n        match list with \n        | Empty -> result \n        | Node(value, containedList) -> iter(containedList, op.Operation(value, result))\n    iter(list, op.Identity)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-23T18:13:13+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-10-a-gentle-introduction-to-the-lambda-calculus/",
    "title": "A Gentle Introduction to the Lambda Calculus",
    "description": "An informal introduction to the simplest programming language",
    "author": [
      {
        "name": "Rahul Chhabra",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\nIntroduction\nThe Œª-calculus is the world‚Äôs simplest programming language.\nThe easiest way to ‚Äúobtain‚Äù a simpler version of a language is to\nremove abstractions : - remove objects and classes and namespaces from\nC++ to get C - remove pattern-matching, extension functions and lambdas,\nwhatever the hell inline <reified T, R> means and you\nget Java - remove ownership, lifecycles, and traits from Rust and get\nC++.\nIn all of these examples, I‚Äôve ignored syntactic differences, since\nthey are just that : syntactic.\nTo obtain the Œª-calculus however, we go further : we strip objects,\ntypes, values, and even numbers. We retain one and only one idea : the\nidea of a function.\nAs we shall see, the idea of a function(in the Œª-calculus sense) is\nstrong enough to allow us to do practically anything.\nHistorical Development\nThis still does not answer the basic questions one might have:\n\nWhy care about the Œª-calculus?\n\nThe Œª-calculus is a solution to a problem. So it is important to\nunderstand the problem it solves.\nTODO : Add Historical Development\nPhilosophy\nTODO : Add philosophical implications\nSyntax\nThe Œª-calculus is unimaginably simple in it‚Äôs syntax.\nEverything is an expression. We can then feed ‚Äúvalues‚Äù\n(other expressions) to these expressions for an evaluation.\nLet us consider a few examples.\nThe simple functions\nThe simplest mathematical object in any field is always the identity.\nThe identity matrix, \\(0\\) for\naddition, \\(1\\) for multiplication.\nLet us start with the identity function:\n\\[\n    id = {\\lambda}x.x\n\\]\nHere‚Äôs what this means:\nWe use a \\({\\lambda}\\) to denote\nthe definition of a function.\nWe use \\(x\\) as the name of our\nformal parameter.\nWe use \\(.\\) to denote the\nbeginning of the ‚Äúfunction body‚Äù\nWe use \\(x\\) => the body simply\nreturns our parameter that we had named \\(x\\).\nA natural question comes up : over what sets or\ntypes have we defined this function.\nThe answer is that we haven‚Äôt.\n\\(x\\) is just a symbol. This \\(id\\) function works for\neverything.\nA universal identity function of sorts.\nSo if we put \\(x=5\\) we get \\(5\\) and if we put \\(x=f(n)\\) where \\(f(n)\\) is some function : we get \\(f(n)\\).\nThe Œª-calculus is so simple that the concept of type is not yet\nwell-defined. If we wish to define types, we will have to do that too\nusing functions.\nThe not-so-simple functions\nWhat about multivariate functions, you ask?\nThe Œª-calculus is so simple that we only have single-parameter\nfunctions.\nTo model multivariate functions, we use a trick.\nLet me illustrate.\nConsider the function:\n\\[\n    f(x, y) = x^2 + y^2\n\\]\nThis is a simple multivariate function.\nWhat happens if we put \\(y=3\\)?\nWe get :\n\\[\n    f(x, 3) = 9 + x^2\n\\]\nA single variable function!\nSo the evaluation of ‚Äú\\(n\\)-variate‚Äù\nfunction with a single argument yields an ‚Äú\\((n - 1)\\)-variate‚Äù function.\nThis little trick is known as ‚Äúcurrying‚Äù.\nSo if we define our function above as :\n\\[\n    f = {\\lambda}x.{\\lambda}y.(x^2 + y^2)\n\\]\n\\(f\\) is a function of one variable\n(\\(x\\)) but it also returns a function\nof one variable(a function of \\(y\\))!\nWe also introduce the syntax for actually putting in values to these\nfunctions: \\[\n    (f \\hspace{1mm} 3)\n\\] This yields the function:\n\\[\n    (f \\hspace{1mm} 3) = {\\lambda}y.(9 + y^2)\n\\]\nLet us put \\(y=4\\) in this curried\nfunction:\n\\[\n    ((f \\hspace{1mm} 3) \\hspace{1mm} 4) = 9 + 16=25\n\\]\nFunction application in the Œª-calculus always associates to the left,\nso we may as well write it more concisely as follows:\n\\[\n    (f \\hspace{1mm} 3 \\hspace{1mm} 4) = 25\n\\]\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-23T18:13:13+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-10-logarithmic-multiplication-the-russian-peasants-method/",
    "title": "Logarithmic Multiplication : The Russian peasant's method",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Rahul Chhabra",
        "url": {}
      }
    ],
    "date": "2022-01-24",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nProblem Statement\n\nExponentiation as a\nlinear recursive process\nAnother definition\nof exponentiation\n\nMultiplication as a\nlinear recursive process\nAnother way to\ndefine multiplication\n\nLogarithmic\nComputation of Hyperoperations\n\nIntroduction\nIn this article, we wish to explore multiplication in logarithmic\ntime using only the following operations: 1. addition 2. multiplication\nby 2 3. division by 2\nThis is actually an exercise in the SICP. The SICP was\nthe introductory course in computer science at MIT for many years. It\nhas now been discontinued but all the resources are still available\nonline.\nThe SICP itself has all it‚Äôs programs in a programming language known\nas Lisp. Lisp is not particularly popular but if you‚Äôve ever used a\nlanguage with any of the following: - garbage collection - dynamic\ntyping - self-hosted compiler - higher-order functions\nYou have Lisp to thank. The fact that SICP expects all it‚Äôs exercises\nin Lisp has some profound consequences : in Lisp, there are no\nfor or while loops. Our primary tool is\nrecursion and tail-call optimisations. There‚Äôs also no mutation : if\na is 3 you cannot reset it to any other value.\nBoth of these are now common patterns in functional programming.\nIn the spirit of SICP, we write the code in this article in Kotlin,\nsince Kotlin allows us to write code in a functional style without much\nfuss.\nProblem Statement\nI was reading the SICP when I came across the problem of implementing\nmultiplication using only addition and bitwise shifts such that the\nalgorithm has exactly \\(\\Theta(log(n))\\) time complexity.\nThe actual problem statement was as follows :\n\nThe following multiplication procedure (in which it is assumed that\nour language can only add, not multiply) is analogous to the\nexpt procedure:\n\n(define (* a b)\n    (if (= b 0)\n        0\n        (+ a (* a (- b 1)))))\n\nThis algorithm takes a number of steps that is linear in\nb. Now suppose we include, together with addition,\noperations double, which doubles an integer, and\nhalve, which divides an (even) integer by 2. Using these,\ndesign a multiplication procedure analogous to fast-expt\nthat uses a logarithmic number of steps.\n\nThe Lisp code is better translated to the following Kotlin:\nfun multiply(a: Int, b: Int) = when (b) {\n    0 -> 0\n    else -> a + multiply(a, b - 1)\n}\nBehind the cryptic Lisp syntax, SICP wants us to make the following\nobservations :\nIt is possible to define a linear recursive formulation of\nmultiplication in terms of addition\nIt is similar enough to exponentiation for us to apply\ndivide-and-conquer and compute it in \\(\\Theta(log(n))\\)\nExponentiation as\na linear recursive process\nExponentiation may be defined recursively as follows:\n\\[ a^b = a * a^{b-1} \\]\n\\[ a^0 = 1 \\]\nThis can easily be implemented in code :\nfun Int.exp(n: Int) = when (n) {\n    0 -> 1\n    else -> this * this.exp(n - 1)\n}\nWe can also implement this tail-recursively:\ntailrec fun Int.expIter(n: Int, answer: Int) = when (n) {\n    0 -> answer\n    else -> this.expIter(n - 1, answer * this)\n}  \nfun Int.exp(n: Int) = this.expIter(n, 1)\nAnother definition of\nexponentiation\nIf we make the observation that\n\\[a^b = {(a^{\\frac{b}{2}})}^2\\]\nwe can redefine exponentiation as follows:\n\\[\n    a ^ b = (a ^ {\\frac{b}{2}})^2, b = 2k, k \\in \\mathbb{Z}\n\\]\n\\[\n    a ^ b = a * (a ^ {\\frac{b - 1}{2}})^2, b = 2k+1, k \\in \\mathbb{Z}\n\\]\nNow we if we write :\ninline val Int.isEven: Boolean\n    get() = (this % 2 ) == 0\ninline fun Int.square(): Int = this * this\nfun Int.exp(n: Int) = when {\n    n == 0 -> 1\n    n.isEven -> this.exp(n / 2).square()\n    else -> this * this.exp((n - 1) / 2).square()\n}\nObserve that this time exponentiation will terminate in \\(\\Theta(log(n))\\) time.\nWe can again convert this to a tail-recursive formulation:\ninline val Int.isEven: Boolean\n    get() = (this % 2 ) == 0\ninline fun Int.square(): Int = this * this\ntailrec fun Int.expIter(n: Int, k: Int) = when {\n    n == 1 -> this * k \n    n.isEven -> this.square().expIter(n / 2, k)\n    else -> this.square().expIter((n - 1) / 2, this * k)\n}\nfun Int.exp(n: Int) = this.expIter(n, 1)\nI highly suggest you try dry-running some of these tail-recursive\nformulations, they‚Äôre not intuitive at first-look.\nHopefully we‚Äôve built enough intuition for exponentiation to extend\nit to multiplication.\nMultiplication as\na linear recursive process\nWe may define multiplication as follows:\n\\[\n    a*b = a + a*(b - 1)\n\\]\n\\[\n    a*0 = 0\n\\]\nAssuming \\(+\\) is well-defined and\n\\(0\\) is the identity of \\(+\\).\nLet‚Äôs code it out:\nfun Int.multiply(b: Int) = when (b) {\n    0 -> 0\n    else -> this + this.multiply(b - 1)\n}\nAnd also, a tail recursive version, for good measure :\ntailrec fun Int.multiplyIter(b: Int, answer: Int) = when (b) {\n    0 -> answer\n    else -> this.multiplyIter(b - 1, answer + this)\n}\nfun Int.multiply(b: Int) = this.multiplyIter(b, 0)\nAnother way to define\nmultiplication\nNow, the time has finally come for us to provide a definition for\nmultiplication that will allow us to compute it in logarithmic time!\n\\[\n    a * b = double(a*halve(b)), b = 2k, k \\in \\mathbb{Z}\n\\]\n\\[\n    a * b = a + double(a*halve(b - 1)), b = 2k + 1, k \\in \\mathbb{Z}\n\\]\nLet‚Äôs code it out :\ninline fun Int.doubled() = this * 2\ninline fun Int.halved() = this / 2\ninline val Int.isEven = this % 2 == 0\nfun Int.multiply(b: Int) = when (b) {\n    0 -> 0\n    isEven -> this.multiply(b.halved()).doubled()\n    else -> this + this.multiply((b - 1).halved()).doubled()\n}\nAgain, we also present a tail-recursive formulation:\ntailrec fun Int.multiplyIter(b: Int, answer) = when (b) {\n    0 -> answer\n    isEven -> this.doubled().multiplyIter(b.halved(), answer)\n    else -> this.doubled().multiplyIter((b - 1).halved(), answer + this)\n}\nLogarithmic\nComputation of Hyperoperations\nThe urge to generalise is very strong.\nTo motivate this generalisation, let us introduce the notion of the\nsuccessor function\n\\[ S(n) = n + 1, \\forall n \\in\n\\mathbb{N}\\]\nNow, let us say we compose \\(S(n)\\)\nwith itself \\(k\\) times :\n\\[ S_k(n) = S(S(S(...S(n))..)\n          = (((((n + 1) + 1) + 1)..) + 1)\n          = n + k\\]\nWe may think of this as a two argument function \\(G_{1}(a, b) = a+b\\). We call it \\(G_1\\) intentionally to allow us to then\nwrite: \\[\n    G_{1}(a, b) = G_0^{b}(a)\n\\] where \\(G_0\\) is the\nsuccessor function and the exponentiation notation is used to denote\nself-composition \\(b\\) times.\nThis notion of repeated composition can be extended to form the idea\nof ‚Äúhyperoperations‚Äù.\nMore formally,\n\\[\n    G_{n}(a,b) = G_{n - 1}(a, G_n(a, b - 1))\n\\]\nWith the following base cases:\n\\[\nG_0(a,b) = b+1\n\\]\n\\[\nG_1(a,0) = a\n\\]\n\\[\nG_2(a,0) = 0\n\\]\n\\[\nG_n(a,0) = 1, n \\geq 3\n\\]\nLet‚Äôs confirm that this indeed is a generalisation! Consider \\(n=2\\)\n\\[G_2(a, b) = G_1(a, G_2(a, b -\n1))\\]\nBut \\(G_1\\) is just addition:\n\\[G_2(a, b) = a + G_2(a, b -\n1)\\]\nThis will expand into \\(b\\)\niterations of addition with \\(a\\), that\nis, \\(ab\\).\nThis sequence of functions is known as the\nhyperoperations.\nNow, we can write:\n\\[\nG_k = G_k(2, G_k(a, \\frac{b}{2})), b=2k, k \\in \\mathbb{Z}\n\\]\n\\[\nG_k = G_{k-1}(a, G_k(2, G_k(a, \\frac{b-1}{2}))), b=2k+1, k \\in\n\\mathbb{Z}\n\\]\nWith this definition of hyperoperations, it is possible to compute\nany hyperoperation in \\(\\Theta(log(n))\\) time complexity!\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-23T18:13:13+00:00",
    "input_file": {}
  }
]
