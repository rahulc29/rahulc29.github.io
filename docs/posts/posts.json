[
  {
    "path": "posts/2023-07-18-the-theory-of-streams/",
    "title": "The theory of Streams",
    "description": "Streams are the categorical dual to lists - they are the terminal coalgebra of the same functor that lists are algebras of.",
    "author": [
      {
        "name": "Rahul Chhabra",
        "url": "https://rahulc29.github.io/"
      }
    ],
    "date": "2023-07-18",
    "categories": [],
    "contents": "\n\nContents\nCeremonial imports :\nThe Even Odd Merger Theorem\nPredicates and Temporal Logic\n\n\nCeremonial imports :\n{-# OPTIONS --cubical --guardedness #-}\nopen import Cubical.Core.Everything\nopen import Cubical.Foundations.Prelude\nopen import Cubical.Data.Maybe\nopen import Cubical.Data.Sigma\n\nLet us define streams (they are the final coalgebra of a certain functor).\nrecord Stream (A : Set) : Set where\n  coinductive\n  field\n    head : A\n    tail : Stream A\nopen Stream\n\nLet us define the function that extracts even indexed elements.\nevens : ∀ {A : Set} → Stream A → Stream A\nhead (evens x) = head x\ntail (evens x) = evens (tail (tail x))\n\nSimilary, we construct the function that extracts odd indexed elements.\nodds : ∀ {A : Set} → Stream A → Stream A\nhead (odds x) = head (x .tail)\ntail (odds x) = odds (tail (tail x))\n\nWe now define bisimulation relations for streams\nrecord _≈_ {A : Set} (x y : Stream A) : Set where\n  coinductive\n  field\n    head-≡ : head x ≡ head y\n    tail-≈ : tail x ≈ tail y\nopen _≈_\n\nOkay, now we have a proof of coinductive extensionality!\n≈→≡ : ∀ {A : Set} → ∀ {xs ys : Stream A} → xs ≈ ys → xs ≡ ys\nhead (≈→≡ xs≈ys i) = (head-≡ xs≈ys) i\ntail (≈→≡ xs≈ys i) = ≈→≡ (tail-≈ xs≈ys) i\n\n≡→≈ : ∀ {A : Set} → ∀ {xs ys : Stream A} → xs ≡ ys → xs ≈ ys\nhead-≡ (≡→≈ xs≡ys) = λ i → head (xs≡ys i)\ntail-≈ (≡→≈ xs≡ys) = ≡→≈ (λ i → tail (xs≡ys i))\n\nNow we must show our equation.\n{-# TERMINATING #-}\nevens∙tail≈odds : ∀ {A : Set} → ∀ (xs : Stream A) → evens (tail xs) ≈ odds xs\nhead-≡ (evens∙tail≈odds xs) = refl\ntail-≈ (evens∙tail≈odds xs) = ≡→≈ ((≈→≡ (evens∙tail≈odds (tail (tail xs)))) ∙ (sym refl))\n\nevens∙tail≡odds : ∀ {A : Set} → ∀ (xs : Stream A) → evens (tail xs) ≡ odds xs\nevens∙tail≡odds xs = ≈→≡ (evens∙tail≈odds xs)\n\nNow let us define the merge operation for streams. The merge operation takes the first element from the left stream and the second element from the second stream.\nmerge : ∀ {A : Set} → Stream A → Stream A → Stream A\nhead (merge x y) = x .head\ntail (merge x y) = merge y (x .tail)\n\nThe Even Odd Merger Theorem\nThe time has come for us to make our biggest theorem yet.\nFor the proof, we need a simple lemma :\nWe will start with the bisimulation lemma and the actual proof would be a simple application of extensionality.\n-- merge-evens-odds-id : ∀ {A : Set} → ∀ (x : Stream A) → merge (evens x) (odds x) ≈ x\n-- head-≡ (merge-evens-odds-id x) = refl\n-- tail-≈ (merge-evens-odds-id x) = ≡→≈ {!? ∙∙ (merge-evens-odds-id (tail x)) ∙∙ refl!}\n\nPredicates and Temporal Logic\nWe define a predicate as a family of types indexed by the state space.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-08-09T09:32:15+05:30",
    "input_file": {}
  },
  {
    "path": "posts/2023-07-10-using-fixedpoint-combinators-to-implement-recursion/",
    "title": "Using fixedpoint combinators to implement recursion",
    "description": "Implementing recursion was a major challenge while writing my interpreter so I used the oldest trick in the book : I didn't actually implement recursion. One implementation of recursive procedures that is suggested by Friedmann's [Essentials of Programming Languages](https://eopl3.com/preface.html) involved many complications and did not reveal anything about the deeper mathematical structure at play. Besides, to be very honest, I thought the implementation was ugly. I had read about fixedpoint combinators before and knew that it was possible to implement recursion using them - so I did. Here I have documented my journey.",
    "author": [
      {
        "name": "Rahul Chhabra",
        "url": "https://rahulc29.github.io/"
      }
    ],
    "date": "2022-08-26",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nA cute sounding but essentially meaningless quote\nRecursion is nameless\nRecursion as an equation\nRecursion is defined recursively\nRecursive functions as fixpoints?\nFinding fixpoints - the mathematics\nTechnical Jargon\nA monotone sequence of approximations\n\nFinding fixpoints - the computer science\nCurry’s Y Combinator\nActually implementing the recursion in our language\n\nConclusion\nReferences\n\nIntroduction\nOne implementation of recursive procedures that is suggested by Friedmann’s Essentials of Programming Languages involved many complications and did not reveal anything about the deeper mathematical structure at play. Besides, to be very honest, I thought the implementation was ugly. I had read about fixedpoint combinators before and knew that it was possible to implement recursion using them - so I did. Here I have documented my journey.\nA cute sounding but essentially meaningless quote\n\nIn order to understand unbounded recursion, one must understand unbounded recursion.\n\nRecursion is nameless\nThe implementation of recursion in LETREC is whack at worst and whack at best.\nHere’s factorial, for example :\n((fix (lambda (self)\n                (lambda (n)\n                  (if (zero? n)\n                      1\n                      (* n (self (- n 1))))))) 5)\n; reduces to 120 \nFor starters, LETREC allows recursive definitions without names. Secondly, what on earth is this fix form?\nAm I admitting to the fact that the way I’ve implemented recursion requires fixing?\nAlso, what is this self? How can a procedure take itself as it’s parameter? That makes no sense!\nRecursion as an equation\nIn order to justify my case of a very whacky implementation of recursion (the honest justification is laziness but using abstract mathematics makes it look otherwise)\nI will start with the classic example of a while loop.\nLet us say we have a programming language like follows :\nbool-expr ::= true | false | bool and bool | bool or bool | not bool \nint-expr ::= integer | int-expr + int-expr | int-expr * int-expr | int-expr - int-expr \nexpr ::= bool-expr | int-expr \nassignment ::= variable <- expr \ncommand ::= skip | assignment | if bool-expr then command else command \n                 | while bool-expr do command | command ; command \nBasically integers, booleans, if-else, assignments and while loops and a skip command that skips over to the next operation. (think empty semicolon ; in langauge like C++)\nObserve that while loops would, then, satisfy the following equation :\nwhile B do C = if B then { C ; while B do C }  \n               else skip \nParametrising over B (B ranges over bool-expr) and C (ranges over command) we get that the while form \\(W_{B,C}\\) is of the form\n\\[\n  W_{B,C} = \\phi(W_{B,C})\n\\]\nwhere \\(\\phi\\) takes any form f to the if B then { C ; f } else skip form.\nMore formally, while is completely defined by the above equation and we say that \\(W_{B,C}\\) is the fixpoint of \\(\\phi_{B,C}\\). Such equations are called fixpoint equations.\nWe have an entire field of mathematics dedicated to the study of fixpoints - domain theory. It should be no surprise that since recursion is defined mathematically using\nfixpoint equations; domain theory is a favourite subject of computer scientists.\nRecursion is defined recursively\nLet us go to the definition of factorial in a classical programming language like Python :\ndef fact(n):\n  if n == 0: \n    return 1 \n  else: \n    return n * fact(n - 1)\nObserve that in this definition is implicit the fact that for fact(n) to be well-defined we need a proof of termination of fact(n - 1).\nIt should also be evident that while not explicitly parametrised over some form of self, this definition still implicitly adds fact to the\nenvironment in which it is defined.\nAny implementation of a programming langauge is inevitably written in some langauge (the metalanguage) and practically every metalanguage will have\nrecursion. This allows the implementor to exploit the recursion already present in the metalanguage to define the operational semantics of recursion\nin the object language.\nRecursive functions as fixpoints?\nIf any recursive definition can be written as a fixpoint equation and doing so allows us to talk about recursion without circular definitions -\nthe natural question to ask is what about recursive functions? The ubiquitous form of recursion must also be described by this.\nIn order to see the sort of equation that defines recursive functions, let us make a few things more explicit in our older factorial function :\ndef fact(self, n):  \n  if n == 0 : \n    return 1\n  else : \n    return n * self(self, n - 1)\nObserve that if self is only capable of computing the factorial of \\(n - 1\\) that too is good enough - we can compute the factorial of \\(n\\) through that.\nSo what we have established is that given a function that can compute the factorial of \\(n - 1\\) it is possible to construct a function that can compute the\nfactorial of \\(n\\).\nThis is starting to sound suspiciously similar to induction!\nWe need a higher-order function - one that takes an approximation of factorial that can compute upto \\(n!\\) and returns a new function\nthat can compute \\((n + 1)!\\).\nThis leads to a certain strategy - recursive functions must be fixpoints of higher-order functions.\nLet us try to actually write down such a function :\ndef fact_combinator(approx, n): \n  def base_case(input):\n    return 1 \n  def inductive_case(input): \n    if input == n :\n      return n * approx(n - 1)\n    return approx(input)\n  if n == 0: \n    return base_case \n  return inductive_case \nIf given the \\(n\\)th approximation to factorial, this function will return the \\((n + 1)\\)th approximation.\nWhat about our first ever approximation? Since this procedure is never invoked we can put it as anything : even a function that doesn’t even terminate!\nIndeed, the “zeroth” approximation is chosen as the “empty function” - the one that doesn’t return for any input whatsoever :\ndef empty(n):\n  return empty(n)\nObserve that using the first ever approximation is a very general trick that will work for any fixpoint equation rather than just the one for factorials!\nFinding fixpoints - the mathematics\nTechnical Jargon\nThe only real theorem we need is that for any given bounded partial order, any monotone sequence converges to a certain limit that can be calculated using\nthe join or the meet operation as appropriate.\nI’m going to be intentionally informal here and that probably sounded like Greek.\nA partial order is essentially a generalisation of the familiar \\(\\le\\) operator for abstract mathematical objects rather than just numbers.\nAn important restriction is that for any two \\(a\\) and \\(b\\) it is not necessary that \\(a \\le b\\) or \\(b \\le a\\).\nA good example is the \\(|\\) “divides” operation on natural numbers. Every number is divisible by itself. If \\(a\\) is divisible by \\(b\\) and \\(b\\) is divisible by\n\\(c\\) then \\(a\\) is divisible by \\(c\\). Note that for any two \\(a\\) and \\(b\\) it is not necessary that \\(a\\) divides \\(b\\) or that \\(b\\) divides \\(a\\) - it might be the case\nthat neither is true.\nAnother good example from computer science is the topological ordering of a graph.\nWhat’s a bounded partial order? Essentially, whatever objects we are considering have a “maximum” and a “minimum” in the sense that \\(\\exists a. \\forall b. a \\le b\\) and \\(\\exists a. \\forall b. b \\le a\\).\nNow, what’s a monotone sequence? Well any sequence \\(a_{n}\\) (finite or infinite) such that either \\(a_{i} \\le a_{i + 1}\\) or \\(a_{i + 1} \\le a_{i}\\).\nWhat we’re saying is that any such sequence always converges. Of course, I have not actually defined convergence. One way of thinking about it is\nthat there is one unique element that to which the elements in our sequence appear to get closer to.\nFor reasons of brevity I will not go too much into the discussion about convergence and instead state the result and we’ll accept it as an axiom.\nAlso, since we will only be talking a particular kind of partial order - the partial order of subsets of a set, I will only talk about that.\nObserve that given two subsets of some set \\(U\\), say \\(A\\) and \\(B\\), the relation \\(\\subseteq\\) is a partial order!\nAlso, the empty set \\({}\\) is the subset of any subset of \\(U\\) and the subset \\(U\\) is the superset of any subset of \\(U\\).\nIt is not just a partial order - it is a bounded partial order!\nNow we will state the theorems in their entire glory.\nAssume that \\(F_{n}\\) is a monotone sequence of subsets of some universe \\(U\\). Specifically, \\(F_{n} \\le F_{n + 1}\\).\nThen we define the limit of this sequence as the countable union of it’s elements :\n\\[\n  \\lim_{n \\to \\infty} F_{n} = \\bigcup_{i = 1}^{\\infty} F_{i}\n\\]\nThis will suffice for our purposes.\nA monotone sequence of approximations\nRemember our old sequence of approximations of the factorial function?\nThis sequence starts with the empty function and we have a combinator \\(\\mathbb{F}\\) that can take the \\(n\\)th approximation and return the \\((n + 1)\\)th approximation (\\(\\mathbb{F}(F_{n}) = F_{n + 1}\\)).\nOne way to model these approximations is as partial functions. Conventional functions have an output defined for any input but partial functions have an output defined\nfor some inputs. Our approximation \\(F_{n}\\) can really only produce outputs for inputs upto \\(n\\).\nSparing some details, this means that \\(F_{n}\\) is a function defined for a subset of the natural numbers (\\({0 \\dots n}\\)) and the base case is defined for the empty set.\nAlso, since functions themselves are sets (of input-output pairs) we can ask whether two functions’ domains are subsets of each other or not. This means that our approximations form\nthe good-old partial ordering we defined in the last section.\nThe function with the empty set as the domain acts as the lower bound and any function with the entire universe (in our case \\(\\mathbb{N}\\)) is the upper bound.\nSo this partial order is also bounded! It is also a subset partial order so our old form applies too!\nI will not prove it but the solution to our recursive equation, our fixpoint to the combinator defined earlier is exactly the limit of this sequence. But\nthis limit is guranteed to exist since the sequence is guranteed to converge!\nSo the factorial function, a function that computes the factorial of any given natural number is precisely defined by the following operation :\n\\[\n  F = \\lim_{n \\to \\infty} F_{n} = \\bigcup_{i = 1}^{\\infty} F_{i}\n\\]\nWow, all this just to define mathematically recursion without actually using recursion. This might look unnecessarily complex (it is) but it’s still\nimportant since this gives a totally new perspective on what recursion is rather than just “stack go brr”.\nFinding fixpoints - the computer science\nDuring the last section we dived into what some might call “heavy mathematics”. It might be difficult to relate it back to CS.\nOur first question is - does the fixpoint always exist? Does any arbitrary recursive definition produce a valid term? (spoiler alert : yes).\nSecondly, if the fixpoint does exist, can we write a computer program to actually compute it? (spoiler alert : yes).\nCurry’s Y Combinator\nThis section assumes some basic knowledge about the untyped lambda calculus.\nWe define the (in)famous Y combinator as follows :\n\\[\n  Y = \\lambda f. ((\\lambda x. f (x x)) (\\lambda x. f (x x)))\n\\]\nThe Y combinator is the most popular of a family fixpoint combinators. A combinator is a lambda term with no free variables. Observe that\nour definition indeed does not contain any free variables. The Y combinator has the property that \\(Y f = f (Y f)\\) for any term \\(f\\).\nThis means that the Y combinator takes any lambda abstraction and returns it’s fixpoint.\nOf course, we have not considered other important questions - is the fixpoint unique? If the fixpoint is not unique, then which fixpoint is returned by the\nY combinator? I’ll leave all this for now.\nLet us take an example : let’s compute the fixpoint of the identity function. The term \\(\\lambda y.y\\) will represent the identity function.\n\\[\\begin{equation}\n\\begin{aligned}\n  Y id \\\\\n  = Y \\lambda y. y  \\\\\n  = \\lambda f. ((\\lambda x. f (x x)) (\\lambda x. f (x x))) \\lambda y.y \\\\\n  = ((\\lambda x. (\\lambda y. y (x x))) (\\lambda x. (\\lambda y. y (x x)))) \\\\\n  = ((\\lambda x. (x x)) (\\lambda x. (x x))) \\\\\n\\end{aligned}\n\\end{equation}\\]\nObserve that the last term is the classic example of a non-terminating infinite loop.\nThis makes sense, we only expected that the Y combinator will produce a well-formed term for any recursive definition; we cannot gurantee that\nthis well-formed term be terminating.\nActually implementing the recursion in our language\nOne might argue that actually computing the fixpoint for a recursive function this way seems to be a bad idea. In this argument, they would be correct.\nActually computing fixpoints would be slow and inefficient. A better idea is to simply use the recursion present in the metalanguage.\nI have taken the call-by-value flavour of the Y-combinator. Scheme has call-by-value semantics and the\nobject language has inherited these semantics.\nThe call-by-value Y-combinator is defined as follows :\n\\[\n  Y = \\lambda f. ((\\lambda x. f (\\lambda y. (x x) y)) (\\lambda x. f (\\lambda y. (x x) y)))\n\\]\nIn the interpreter this is defined as a primitive by evaluating a simple parse tree in the empty environment :\n(define y-combinator\n    (value-of (parse-tree '(lambda (f)\n                             ((lambda (x)\n                                (f (lambda (y)\n                                     ((x x) y))))\n                              (lambda (x)\n                                (f (lambda (y)\n                                     ((x x) y)))))))\n              (empty-env)))\nThis means that fixpoints of recursive functions is computed with the Y combinator.\nConclusion\nI know very well that I’ll have to implement proper fixpoint handling and proper recursion handling since algebraic datatypes require so. Nonetheless, this works\nwell enough for now and I can add a new defrec form as syntactic sugar that reduces to the fix form.\nI have also been very informal and non-rigourous with the mathematics. I will be fixing both of these very soon.\nOne might argue that putting in so much mathematics and extra garbage in this design doc is unnecessary - but this fundamentally misunderstands the philosophy.\nSince the design is motivated entirely by personal laziness, the design doc itself is essentially a glorified shitpost.\nReferences\nFor understanding about the untyped lambda calculus, one may see Type Theory and Formal Proof or Lectures on the Curry-Howard Isomorphism\nFor understanding about general recursion and algebraic datatypes see Pierce’s Types and Programming Languages\nFor understanding the mathematics behind recursion see any book on domain theory or programming language semantics.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-08-09T09:39:17+05:30",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-24-the-beauty-and-applicability-of-algebraic-types/",
    "title": "The beauty and applicability of algebraic types",
    "description": "In this article we explore the wide applicability of algebraic types and how to use them in daily life",
    "author": [
      {
        "name": "Rahul Chhabra",
        "url": "https://rahulc29.github.io/"
      }
    ],
    "date": "2022-02-24",
    "categories": [
      "computer science",
      "mathematics"
    ],
    "contents": "\n\nContents\nIntroduction\nSyntax and elementary\nnotions\nLinked Lists as algebraic\ntypes\nElementary\noperations on algebraic linked lists\nList Sizes\nSum and\nproduct of an algebraic linked list\n\n\nBinary Search\nTrees as algebraic types\n\nIntroduction\nThe notion of a type is particularly important in\nprogramming. A significant portion of online discourse regarding the\nmerits and demerits of a given programming language comes down to if the\nlanguage’s type system:\nPython is dynamic and strong\nJavaScript is dynamic and weak (3 ==\n\"3\" but 3 !== \"3\")\nC is static and weak\nC++ and Java are static and strong\nKotlin, Rust and Haskell are static and very\nstrong\nOf all the programming languages aforementioned, only the last three\ncontain a rather powerful means of creating and composing types known as\nalgebraic types. It would be unfair to not mention the fact\nthat Java and C++\nare both proposing to add algebraic types to their type systems.\nSyntax and elementary\nnotions\nAt a very fundamental level, the notion of doing “algebra” with types\ncomes down to the following two operations :\nDisjoint Union : the union of two disjoint types\nensures that if we are given any instance of that type, we know it’s\n“parent type”\nCartesian Product : the Cartesian Product of types\nA and B is the tuple (A,\nB)\nMost programming languages explicitly or implicitly contain the\nnotion of the Cartesian product.\nThis is all very abstract, so let’s try to make something using only\nthese operations : a linked list!\nLinked Lists as algebraic\ntypes\nConsider the following (singly) linked list :\n1 -> 2 -> 3 -> 4 -> 5\nObserve that any node can be thought of as containing\nanother linked list inside it, as in :\n1 -> (2 -> (3 -> (4 -> 5))\nWhat about the 5 though?\nLet us play mathematician and assume that the node\ncontaining 5 contains the empty list inside\nit.\nIs this legal?\nWell, we make the rules around here! 🤫\nSo our original list now becomes :\n1 -> (2 -> (3 -> (4 -> (5 -> []))))\nwhere [] represents the empty list.\nWhat is the point of doing that however?\nWe can now make the following claim, any linked list can be\neither of two things :\nEmpty : if the list is empty, it is\n[]\nNode : a node will have a value (an\nint in this example), and another list, this other list\nitself follows these same constraints obviously\nI’d like to know throw some terminology : the type of\nLinkedList may be defined as the disjoint union of\nthe EmptyList type and the ListNode type.\nMake another observation : the ListNode itself is the\nCartesian product of int and the\nLinkedList type.\nLet us write this down in code, shall we?\ntype LinkedList = Empty | Node of (int * LinkedList)\nLet’s break this down : a LinkedList is either an\nEmpty instance, in which case it holds no further\ninformation (this definition automatically implies that there is\nonly one empty list) or it is a Node consisting of\nthe Cartesian product of int and\nLinkedList\nNow that we have a linked list, let us try to do things with it!\nElementary\noperations on algebraic linked lists\nList Sizes\nThe first problem we will tackle is to compute the length of our\nlinked list. With out algebraic defintion, this is practically a\njoke:\nEmpty : define the size to be 0\nNode : define the size to be 1 added\nto the size of the contained list\nLet us write this down in code :\nlet rec size list = \n    match list with \n    | Empty -> 0 \n    | Node(value, containedList) -> 1 + size(containedList)\nThe match and with is special syntax used\nfor dealing with algebraic types called pattern matching. Every\nlanguage that provides algebraic types also provides some variant of\npattern matching.\nAs an example, let us take our old example and dry run it.\nWe have the list :\nlet a = 1 -> (2 -> (3 -> (4 -> (5 -> []))))\nlet b = a.tail // (2 -> (3 -> (4 -> (5 -> []))))\nlet c = b.tail // (3 -> (4 -> (5 -> [])))\nlet d = c.tail // (4 -> (5 -> []))\nlet e = d.tail // (5 -> [])\nlet f = e.tail // []\nWe have to compute size(a)\nThe execution would look something like this :\nsize(a)\n= 1 + size(b)\n= 1 + 1 + size(c)\n= 1 + 1 + 1 + size(d)\n= 1 + 1 + 1 + 1 + size(e)\n= 1 + 1 + 1 + 1 + 1 + size(f)\n= 1 + 1 + 1 + 1 + 1 + 0 \n= 5\nNote that this algorithm takes stack space that is linear\nwith the size of the list.\nHere’s an alternative implementation :\nlet size list = \n    // local function\n    let iter list result = \n        match list with \n        | Empty -> result \n        | Node(value, containedList) -> iter(containedList, result + 1)\n    iter(list, 0)\nThe stack space of this algorithm is constant with the size of the\nlist.\nHere’s a challenge for you : can you implement this algorithm in C++\nwithout using recursion but while loops instead? (a close approximation\nto this can be achieved by using an enum class and\nperforming switch case)\nSum and product of\nan algebraic linked list\nLet us compute the sum of an algebraic linked list.\nI urge you to come up with a solution yourself first!\nHere’s one way to do it :\nlet rec sum list = \n    match list with \n    | Empty -> 0 \n    | Node(value, containedList) -> value + sum(containedList)\nBeauty lies in the eyes of the beholder, but you need an eye checkup\nif you don’t consider this beautiful!\nVery similarly, we can compute the product of a list :\nlet rec product list = \n    match list with \n    | Empty -> 1 \n    (* this might not make much sense; \n    but is actually the same thing as saying that the sum of the empty list is 0 *)\n    | Node(value, containedList) -> value * product(containedList)\nThe two codes are awfully similar, let us generalise :\nwe need an identity to associate with the empty list\nwe need an operation to associate the value from the\ncomputation to our node’s value\nLet us write a general procedure that achieves this :\nlet rec reduce list operation identity = \n    match list with \n    | Empty -> identity \n    | Node(value, containedList) -> operation(value, containedList)\nThis is nice and all but a function with three arguments of which two\nclearly share a semantic relationship is extremely ugly!\nHow do we fix this?\nThe Cartesian product!\ntype BinaryOperation =  int * (int -> int -> int)\nThe int -> int -> int syntax is used to denote a\nfunction that takes two ints and returns an\nint.\nIf you are curious about the origin of such a syntax : look up the\nconcept of currying, you will probably enjoy it!\nThis is still not particularly descriptive : the type does not talk\nabout what the int and int -> int -> int\nare for.\nTo mitigate that, we can give them names :\ntype BinaryOperation = \n    { Identity: int,\n      Operation: int -> int -> int }\nThis might look like structs from other languages such\nas C/C++ or Rust.\nIt acts very similar to a struct and is called a\nrecord.\nThe only place in which records differ from structs is\nthat records can be natively recursive while structs can be\nimplemented recursively using some kind of pointers.\nI say “some kind of” because both C++ and Rust allow for very exotic\nkinds of pointers that would all work; think\nstd::unique_ptr<T>,\nstd::shared_ptr<T>,\nstd::weak_ptr<T>.\nIn Rust, one has even more control : for example, with\nBox<T> one can enforce that T\nbe on the heap. No such analog exists in C++.\nI digress, let us reimplement our reduce function!\nlet rec reduce list op = \n    match list with \n    | Empty -> op.Identity\n    | Node(value, containedList) -> op.Operation(value, reduce(containedList, op))\nlet Addition = BinaryOperation {\n    Identity : 0,\n    Operation : fun a b -> (a + b)\n}\nlet Multiplication = BinaryOperation {\n    Identity : 1,\n    Operation : fun a b -> (a * b)\n}\nlet sum list = reduce(list, Addition)\nlet product list = reduce(list, Multiplication)\nThis is still not reduces final form.\nObserve that the stack space grows linearly with list size.\nHere’s a mitigation\nlet reduce list op = \n    let rec iter list result = \n        match list with \n        | Empty -> result \n        | Node(value, containedList) -> iter(containedList, op.Operation(value, result))\n    iter(list, op.Identity)\nI’d like to ask for your patience once again : we do not\nneed the list to be defined only for ints.\nOur reduce function should also work for, say, string\nconcetation (\"\" acts as the identity).\nThe changes in our type definitions are trivial :\ntype LinkedList<'a> = Empty | Node of ('a * LinkedList<'a>)\ntype BinaryOperation<'a> = \n    { Identity : 'a,\n      Operation : 'a -> 'a -> 'a }\nA lot has happened here : let us break it down.\nThe 'a is called a type parameter. We can put\n'a = int to re-obtain the list we have been studying till\nnow, int is then called a type argument.\nEssentially, we have generalised our list to accomadate more than one\ntype.\nIn the lingo, we say that the type LinkedList<'a>\nis polymorphic over all 'a, and that\nLinkedList<int> is a monomorphisation for\n'a = int.\nA little bit of trivia : an operation for which an identity,\nassociativity (a * b * c is always unambigous) and it is\nguranteed that a * b is always of the same type as\na and b is called a monoidal\noperation.\nBinary Search Trees as\nalgebraic types\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-18T12:56:59+05:30",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-10-logarithmic-multiplication-the-russian-peasants-method/",
    "title": "Logarithmic Multiplication : The Russian peasant's method",
    "description": "In this article we explore the Russian peasant's method to compute the multiplication of two numbers and generalise it to hyperoperations.",
    "author": [
      {
        "name": "Rahul Chhabra",
        "url": {}
      }
    ],
    "date": "2022-01-24",
    "categories": [
      "computer science",
      "mathematics"
    ],
    "contents": "\n\nContents\nIntroduction\nProblem Statement\n\nExponentiation as a\nlinear recursive process\nAnother definition\nof exponentiation\n\nMultiplication as a\nlinear recursive process\nAnother way to\ndefine multiplication\n\nLogarithmic\nComputation of Hyperoperations\n\nIntroduction\nIn this article, we wish to explore multiplication in logarithmic\ntime using only the following operations: 1. addition 2. multiplication\nby 2 3. division by 2\nThis is actually an exercise in the SICP. The SICP was\nthe introductory course in computer science at MIT for many years. It\nhas now been discontinued but all the resources are still available\nonline.\nThe SICP itself has all it’s programs in a programming language known\nas Lisp. Lisp is not particularly popular but if you’ve ever used a\nlanguage with any of the following: - garbage collection - dynamic\ntyping - self-hosted compiler - higher-order functions\nYou have Lisp to thank. The fact that SICP expects all it’s exercises\nin Lisp has some profound consequences : in Lisp, there are no\nfor or while loops. Our primary tool is\nrecursion and tail-call optimisations. There’s also no mutation : if\na is 3 you cannot reset it to any other value.\nBoth of these are now common patterns in functional programming.\nIn the spirit of SICP, we write the code in this article in Kotlin,\nsince Kotlin allows us to write code in a functional style without much\nfuss.\nProblem Statement\nI was reading the SICP when I came across the problem of implementing\nmultiplication using only addition and bitwise shifts such that the\nalgorithm has exactly \\(\\Theta(log(n))\\) time complexity.\nThe actual problem statement was as follows :\n\nThe following multiplication procedure (in which it is assumed that\nour language can only add, not multiply) is analogous to the\nexpt procedure:\n\n(define (* a b)\n    (if (= b 0)\n        0\n        (+ a (* a (- b 1)))))\n\nThis algorithm takes a number of steps that is linear in\nb. Now suppose we include, together with addition,\noperations double, which doubles an integer, and\nhalve, which divides an (even) integer by 2. Using these,\ndesign a multiplication procedure analogous to fast-expt\nthat uses a logarithmic number of steps.\n\nThe Lisp code is better translated to the following Kotlin:\nfun multiply(a: Int, b: Int) = when (b) {\n    0 -> 0\n    else -> a + multiply(a, b - 1)\n}\nBehind the cryptic Lisp syntax, SICP wants us to make the following\nobservations :\nIt is possible to define a linear recursive formulation of\nmultiplication in terms of addition\nIt is similar enough to exponentiation for us to apply\ndivide-and-conquer and compute it in \\(\\Theta(log(n))\\)\nExponentiation as\na linear recursive process\nExponentiation may be defined recursively as follows:\n\\[ a^b = a * a^{b-1} \\]\n\\[ a^0 = 1 \\]\nThis can easily be implemented in code :\nfun Int.exp(n: Int) = when (n) {\n    0 -> 1\n    else -> this * this.exp(n - 1)\n}\nWe can also implement this tail-recursively:\ntailrec fun Int.expIter(n: Int, answer: Int) = when (n) {\n    0 -> answer\n    else -> this.expIter(n - 1, answer * this)\n}  \nfun Int.exp(n: Int) = this.expIter(n, 1)\nAnother definition of\nexponentiation\nIf we make the observation that\n\\[a^b = {(a^{\\frac{b}{2}})}^2\\]\nwe can redefine exponentiation as follows:\n\\[\n    a ^ b = (a ^ {\\frac{b}{2}})^2, b = 2k, k \\in \\mathbb{Z}\n\\]\n\\[\n    a ^ b = a * (a ^ {\\frac{b - 1}{2}})^2, b = 2k+1, k \\in \\mathbb{Z}\n\\]\nNow we if we write :\ninline val Int.isEven: Boolean\n    get() = (this % 2 ) == 0\ninline fun Int.square(): Int = this * this\nfun Int.exp(n: Int) = when {\n    n == 0 -> 1\n    n.isEven -> this.exp(n / 2).square()\n    else -> this * this.exp((n - 1) / 2).square()\n}\nObserve that this time exponentiation will terminate in \\(\\Theta(log(n))\\) time.\nWe can again convert this to a tail-recursive formulation:\ninline val Int.isEven: Boolean\n    get() = (this % 2 ) == 0\ninline fun Int.square(): Int = this * this\ntailrec fun Int.expIter(n: Int, k: Int) = when {\n    n == 1 -> this * k \n    n.isEven -> this.square().expIter(n / 2, k)\n    else -> this.square().expIter((n - 1) / 2, this * k)\n}\nfun Int.exp(n: Int) = this.expIter(n, 1)\nI highly suggest you try dry-running some of these tail-recursive\nformulations, they’re not intuitive at first-look.\nHopefully we’ve built enough intuition for exponentiation to extend\nit to multiplication.\nMultiplication as\na linear recursive process\nWe may define multiplication as follows:\n\\[\n    a*b = a + a*(b - 1)\n\\]\n\\[\n    a*0 = 0\n\\]\nAssuming \\(+\\) is well-defined and\n\\(0\\) is the identity of \\(+\\).\nLet’s code it out:\nfun Int.multiply(b: Int) = when (b) {\n    0 -> 0\n    else -> this + this.multiply(b - 1)\n}\nAnd also, a tail recursive version, for good measure :\ntailrec fun Int.multiplyIter(b: Int, answer: Int) = when (b) {\n    0 -> answer\n    else -> this.multiplyIter(b - 1, answer + this)\n}\nfun Int.multiply(b: Int) = this.multiplyIter(b, 0)\nAnother way to define\nmultiplication\nNow, the time has finally come for us to provide a definition for\nmultiplication that will allow us to compute it in logarithmic time!\n\\[\n    a * b = double(a*halve(b)), b = 2k, k \\in \\mathbb{Z}\n\\]\n\\[\n    a * b = a + double(a*halve(b - 1)), b = 2k + 1, k \\in \\mathbb{Z}\n\\]\nLet’s code it out :\ninline fun Int.doubled() = this * 2\ninline fun Int.halved() = this / 2\ninline val Int.isEven = this % 2 == 0\nfun Int.multiply(b: Int) = when (b) {\n    0 -> 0\n    isEven -> this.multiply(b.halved()).doubled()\n    else -> this + this.multiply((b - 1).halved()).doubled()\n}\nAgain, we also present a tail-recursive formulation:\ntailrec fun Int.multiplyIter(b: Int, answer) = when (b) {\n    0 -> answer\n    isEven -> this.doubled().multiplyIter(b.halved(), answer)\n    else -> this.doubled().multiplyIter((b - 1).halved(), answer + this)\n}\nLogarithmic\nComputation of Hyperoperations\nThe urge to generalise is very strong.\nTo motivate this generalisation, let us introduce the notion of the\nsuccessor function\n\\[ S(n) = n + 1, \\forall n \\in\n\\mathbb{N}\\]\nNow, let us say we compose \\(S(n)\\)\nwith itself \\(k\\) times :\n\\[ S_k(n) = S(S(S(...S(n))..)\n          = (((((n + 1) + 1) + 1)..) + 1)\n          = n + k\\]\nWe may think of this as a two argument function \\(G_{1}(a, b) = a+b\\). We call it \\(G_1\\) intentionally to allow us to then\nwrite: \\[\n    G_{1}(a, b) = G_0^{b}(a)\n\\] where \\(G_0\\) is the\nsuccessor function and the exponentiation notation is used to denote\nself-composition \\(b\\) times.\nThis notion of repeated composition can be extended to form the idea\nof “hyperoperations”.\nMore formally,\n\\[\n    G_{n}(a,b) = G_{n - 1}(a, G_n(a, b - 1))\n\\]\nWith the following base cases:\n\\[\nG_0(a,b) = b+1\n\\]\n\\[\nG_1(a,0) = a\n\\]\n\\[\nG_2(a,0) = 0\n\\]\n\\[\nG_n(a,0) = 1, n \\geq 3\n\\]\nLet’s confirm that this indeed is a generalisation! Consider \\(n=2\\)\n\\[G_2(a, b) = G_1(a, G_2(a, b -\n1))\\]\nBut \\(G_1\\) is just addition:\n\\[G_2(a, b) = a + G_2(a, b -\n1)\\]\nThis will expand into \\(b\\)\niterations of addition with \\(a\\), that\nis, \\(ab\\).\nThis sequence of functions is known as the\nhyperoperations.\nNow, we can write:\n\\[\nG_k = G_k(2, G_k(a, \\frac{b}{2})), b=2k, k \\in \\mathbb{Z}\n\\]\n\\[\nG_k = G_{k-1}(a, G_k(2, G_k(a, \\frac{b-1}{2}))), b=2k+1, k \\in\n\\mathbb{Z}\n\\]\nWith this definition of hyperoperations, it is possible to compute\nany hyperoperation in \\(\\Theta(log(n))\\) time complexity!\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-18T12:56:59+05:30",
    "input_file": {}
  }
]
