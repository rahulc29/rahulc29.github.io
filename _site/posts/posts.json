[
  {
    "path": "posts/2022-02-10-a-gentle-introduction-to-the-lambda-calculus/",
    "title": "A Gentle Introduction to the Lambda Calculus",
    "description": "An informal introduction to the simplest programming language",
    "author": [
      {
        "name": "Rahul Chhabra",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\nIntroduction\nThe λ-calculus is the world’s simplest programming language.\nThe easiest way to “obtain” a simpler version of a language is to\nremove abstractions : - remove objects and classes and namespaces from\nC++ to get C - remove pattern-matching, extension functions and lambdas,\nwhatever the hell inline <reified T, R> means and you\nget Java - remove ownership, lifecycles, and traits from Rust and get\nC++.\nIn all of these examples, I’ve ignored syntactic differences, since\nthey are just that : syntactic.\nTo obtain the λ-calculus however, we go further : we strip objects,\ntypes, values, and even numbers. We retain one and only one idea : the\nidea of a function.\nAs we shall see, the idea of a function(in the λ-calculus sense) is\nstrong enough to allow us to do practically anything.\nHistorical Development\nThis still does not answer the basic questions one might have:\n\nWhy care about the λ-calculus?\n\nThe λ-calculus is a solution to a problem. So it is important to\nunderstand the problem it solves.\nTODO : Add Historical Development\nPhilosophy\nTODO : Add philosophical implications\nSyntax\nThe λ-calculus is unimaginably simple in it’s syntax.\nEverything is an expression. We can then feed “values”\n(other expressions) to these expressions for an evaluation.\nLet us consider a few examples.\nThe simple functions\nThe simplest mathematical object in any field is always the identity.\nThe identity matrix, \\(0\\) for\naddition, \\(1\\) for multiplication.\nLet us start with the identity function:\n\\[\n    id = {\\lambda}x.x\n\\]\nHere’s what this means:\nWe use a \\({\\lambda}\\) to denote\nthe definition of a function.\nWe use \\(x\\) as the name of our\nformal parameter.\nWe use \\(.\\) to denote the\nbeginning of the “function body”\nWe use \\(x\\) => the body simply\nreturns our parameter that we had named \\(x\\).\nA natural question comes up : over what sets or\ntypes have we defined this function.\nThe answer is that we haven’t.\n\\(x\\) is just a symbol. This \\(id\\) function works for\neverything.\nA universal identity function of sorts.\nSo if we put \\(x=5\\) we get \\(5\\) and if we put \\(x=f(n)\\) where \\(f(n)\\) is some function : we get \\(f(n)\\).\nThe λ-calculus is so simple that the concept of type is not yet\nwell-defined. If we wish to define types, we will have to do that too\nusing functions.\nThe not-so-simple functions\nWhat about multivariate functions, you ask?\nThe λ-calculus is so simple that we only have single-parameter\nfunctions.\nTo model multivariate functions, we use a trick.\nLet me illustrate.\nConsider the function:\n\\[\n    f(x, y) = x^2 + y^2\n\\]\nThis is a simple multivariate function.\nWhat happens if we put \\(y=3\\)?\nWe get :\n\\[\n    f(x, 3) = 9 + x^2\n\\]\nA single variable function!\nSo the evaluation of “\\(n\\)-variate”\nfunction with a single argument yields an “\\((n - 1)\\)-variate” function.\nThis little trick is known as “currying”.\nSo if we define our function above as :\n\\[\n    f = {\\lambda}x.{\\lambda}y.(x^2 + y^2)\n\\]\n\\(f\\) is a function of one variable\n(\\(x\\)) but it also returns a function\nof one variable(a function of \\(y\\))!\nWe also introduce the syntax for actually putting in values to these\nfunctions: \\[\n    (f \\hspace{1mm} 3)\n\\] This yields the function:\n\\[\n    (f \\hspace{1mm} 3) = {\\lambda}y.(9 + y^2)\n\\]\nLet us put \\(y=4\\) in this curried\nfunction:\n\\[\n    ((f \\hspace{1mm} 3) \\hspace{1mm} 4) = 9 + 16=25\n\\]\nFunction application in the λ-calculus always associates to the left,\nso we may as well write it more concisely as follows:\n\\[\n    (f \\hspace{1mm} 3 \\hspace{1mm} 4) = 25\n\\]\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-10T13:26:59+05:30",
    "input_file": "a-gentle-introduction-to-the-lambda-calculus.knit.md"
  },
  {
    "path": "posts/2022-02-10-logarithmic-multiplication-the-russian-peasants-method/",
    "title": "Logarithmic Multiplication : The Russian peasant's method",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Rahul Chhabra",
        "url": {}
      }
    ],
    "date": "2022-01-24",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nProblem Statement\n\nExponentiation as a\nlinear recursive process\nAnother definition\nof exponentiation\n\nMultiplication as a\nlinear recursive process\nAnother way to\ndefine multiplication\n\nLogarithmic\nComputation of Hyperoperations\n\nIntroduction\nIn this article, we wish to explore multiplication in logarithmic\ntime using only the following operations: 1. addition 2. multiplication\nby 2 3. division by 2\nThis is actually an exercise in the SICP. The SICP was\nthe introductory course in computer science at MIT for many years. It\nhas now been discontinued but all the resources are still available\nonline.\nThe SICP itself has all it’s programs in a programming language known\nas Lisp. Lisp is not particularly popular but if you’ve ever used a\nlanguage with any of the following: - garbage collection - dynamic\ntyping - self-hosted compiler - higher-order functions\nYou have Lisp to thank. The fact that SICP expects all it’s exercises\nin Lisp has some profound consequences : in Lisp, there are no\nfor or while loops. Our primary tool is\nrecursion and tail-call optimisations. There’s also no mutation : if\na is 3 you cannot reset it to any other value.\nBoth of these are now common patterns in functional programming.\nIn the spirit of SICP, we write the code in this article in Kotlin,\nsince Kotlin allows us to write code in a functional style without much\nfuss.\nProblem Statement\nI was reading the SICP when I came across the problem of implementing\nmultiplication using only addition and bitwise shifts such that the\nalgorithm has exactly \\(\\Theta(log(n))\\) time complexity.\nThe actual problem statement was as follows :\n\nThe following multiplication procedure (in which it is assumed that\nour language can only add, not multiply) is analogous to the\nexpt procedure:\n\n(define (* a b)\n    (if (= b 0)\n        0\n        (+ a (* a (- b 1)))))\n\nThis algorithm takes a number of steps that is linear in\nb. Now suppose we include, together with addition,\noperations double, which doubles an integer, and\nhalve, which divides an (even) integer by 2. Using these,\ndesign a multiplication procedure analogous to fast-expt\nthat uses a logarithmic number of steps.\n\nThe Lisp code is better translated to the following Kotlin:\nfun multiply(a: Int, b: Int) = when (b) {\n    0 -> 0\n    else -> a + multiply(a, b - 1)\n}\nBehind the cryptic Lisp syntax, SICP wants us to make the following\nobservations :\nIt is possible to define a linear recursive formulation of\nmultiplication in terms of addition\nIt is similar enough to exponentiation for us to apply\ndivide-and-conquer and compute it in \\(\\Theta(log(n))\\)\nExponentiation as\na linear recursive process\nExponentiation may be defined recursively as follows:\n\\[ a^b = a * a^{b-1} \\]\n\\[ a^0 = 1 \\]\nThis can easily be implemented in code :\nfun Int.exp(n: Int) = when (n) {\n    0 -> 1\n    else -> this * this.exp(n - 1)\n}\nWe can also implement this tail-recursively:\ntailrec fun Int.expIter(n: Int, answer: Int) = when (n) {\n    0 -> answer\n    else -> this.expIter(n - 1, answer * this)\n}  \nfun Int.exp(n: Int) = this.expIter(n, 1)\nAnother definition of\nexponentiation\nIf we make the observation that\n\\[a^b = {(a^{\\frac{b}{2}})}^2\\]\nwe can redefine exponentiation as follows:\n\\[\n    a ^ b = (a ^ {\\frac{b}{2}})^2, b = 2k, k \\in \\mathbb{Z}\n\\]\n\\[\n    a ^ b = a * (a ^ {\\frac{b - 1}{2}})^2, b = 2k+1, k \\in \\mathbb{Z}\n\\]\nNow we if we write :\ninline val Int.isEven: Boolean\n    get() = (this % 2 ) == 0\ninline fun Int.square(): Int = this * this\nfun Int.exp(n: Int) = when {\n    n == 0 -> 1\n    n.isEven -> this.exp(n / 2).square()\n    else -> this * this.exp((n - 1) / 2).square()\n}\nObserve that this time exponentiation will terminate in \\(\\Theta(log(n))\\) time.\nWe can again convert this to a tail-recursive formulation:\ninline val Int.isEven: Boolean\n    get() = (this % 2 ) == 0\ninline fun Int.square(): Int = this * this\ntailrec fun Int.expIter(n: Int, k: Int) = when {\n    n == 1 -> this * k \n    n.isEven -> this.square().expIter(n / 2, k)\n    else -> this.square().expIter((n - 1) / 2, this * k)\n}\nfun Int.exp(n: Int) = this.expIter(n, 1)\nI highly suggest you try dry-running some of these tail-recursive\nformulations, they’re not intuitive at first-look.\nHopefully we’ve built enough intuition for exponentiation to extend\nit to multiplication.\nMultiplication as\na linear recursive process\nWe may define multiplication as follows:\n\\[\n    a*b = a + a*(b - 1)\n\\]\n\\[\n    a*0 = 0\n\\]\nAssuming \\(+\\) is well-defined and\n\\(0\\) is the identity of \\(+\\).\nLet’s code it out:\nfun Int.multiply(b: Int) = when (b) {\n    0 -> 0\n    else -> this + this.multiply(b - 1)\n}\nAnd also, a tail recursive version, for good measure :\ntailrec fun Int.multiplyIter(b: Int, answer: Int) = when (b) {\n    0 -> answer\n    else -> this.multiplyIter(b - 1, answer + this)\n}\nfun Int.multiply(b: Int) = this.multiplyIter(b, 0)\nAnother way to define\nmultiplication\nNow, the time has finally come for us to provide a definition for\nmultiplication that will allow us to compute it in logarithmic time!\n\\[\n    a * b = double(a*halve(b)), b = 2k, k \\in \\mathbb{Z}\n\\]\n\\[\n    a * b = a + double(a*halve(b - 1)), b = 2k + 1, k \\in \\mathbb{Z}\n\\]\nLet’s code it out :\ninline fun Int.doubled() = this * 2\ninline fun Int.halved() = this / 2\ninline val Int.isEven = this % 2 == 0\nfun Int.multiply(b: Int) = when (b) {\n    0 -> 0\n    isEven -> this.multiply(b.halved()).doubled()\n    else -> this + this.multiply((b - 1).halved()).doubled()\n}\nAgain, we also present a tail-recursive formulation:\ntailrec fun Int.multiplyIter(b: Int, answer) = when (b) {\n    0 -> answer\n    isEven -> this.doubled().multiplyIter(b.halved(), answer)\n    else -> this.doubled().multiplyIter((b - 1).halved(), answer + this)\n}\nLogarithmic\nComputation of Hyperoperations\nThe urge to generalise is very strong.\nTo motivate this generalisation, let us introduce the notion of the\nsuccessor function\n\\[ S(n) = n + 1, \\forall n \\in\n\\mathbb{N}\\]\nNow, let us say we compose \\(S(n)\\)\nwith itself \\(k\\) times :\n\\[ S_k(n) = S(S(S(...S(n))..)\n          = (((((n + 1) + 1) + 1)..) + 1)\n          = n + k\\]\nWe may think of this as a two argument function \\(G_{1}(a, b) = a+b\\). We call it \\(G_1\\) intentionally to allow us to then\nwrite: \\[\n    G_{1}(a, b) = G_0^{b}(a)\n\\] where \\(G_0\\) is the\nsuccessor function and the exponentiation notation is used to denote\nself-composition \\(b\\) times.\nThis notion of repeated composition can be extended to form the idea\nof “hyperoperations”.\nMore formally,\n\\[\n    G_{n}(a,b) = G_{n - 1}(a, G_n(a, b - 1))\n\\]\nWith the following base cases:\n\\[\nG_0(a,b) = b+1\n\\]\n\\[\nG_1(a,0) = a\n\\]\n\\[\nG_2(a,0) = 0\n\\]\n\\[\nG_n(a,0) = 1, n \\geq 3\n\\]\nLet’s confirm that this indeed is a generalisation! Consider \\(n=2\\)\n\\[G_2(a, b) = G_1(a, G_2(a, b -\n1))\\]\nBut \\(G_1\\) is just addition:\n\\[G_2(a, b) = a + G_2(a, b -\n1)\\]\nThis will expand into \\(b\\)\niterations of addition with \\(a\\), that\nis, \\(ab\\).\nThis sequence of functions is known as the\nhyperoperations.\nNow, we can write:\n\\[\nG_k = G_k(2, G_k(a, \\frac{b}{2})), b=2k, k \\in \\mathbb{Z}\n\\]\n\\[\nG_k = G_{k-1}(a, G_k(2, G_k(a, \\frac{b-1}{2}))), b=2k+1, k \\in\n\\mathbb{Z}\n\\]\nWith this definition of hyperoperations, it is possible to compute\nany hyperoperation in \\(\\Theta(log(n))\\) time complexity!\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-10T13:33:11+05:30",
    "input_file": "logarithmic-multiplication-the-russian-peasants-method.knit.md"
  }
]
